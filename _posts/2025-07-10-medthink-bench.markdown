---
layout: post
title: "Automating expert-level medical reasoning evaluation of large language models"
date: 2025-07-10 00:00:00 +00:00
image: /images/medthink.png
categories: research
author: "Yanwei Jin"
authors: "Shuang Zhou, Wenya Xie, Jiaxi Li, Zaifu Zhan, Meijia Song, Han Yang, Cheyenna Espinoza, Lindsay Welton, Xinnie Mai, <strong>Yanwei Jin</strong>, Zidu Xu, Yuen-Hei Chung, Yiyun Xing, Meng-Han Tsai, Emma Schaffer, Yucheng Shi, Ninghao Liu, Zirui Liu, Rui Zhang"
venue: "arXiv preprint"
arxiv: https://arxiv.org/pdf/2507.07988
---
**MedThink-Bench**: 500-question benchmark for assessing LLMs' medical reasoning with expert rationales. Our **LLM-w-Ref** framework showed smaller models (MedGemma-27B) can surpass larger proprietary ones (OpenAI-o3).
