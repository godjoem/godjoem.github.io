---
layout: post
title: "Automating expert-level medical reasoning evaluation of large language models"
date: 2025-07-10 00:00:00 +00:00
image: /images/medthink.png
categories: research
author: "Yanwei Jin"
authors: "Shuang Zhou, Wenya Xie, Jiaxi Li, Zaifu Zhan, Meijia Song, Han Yang, Cheyenna Espinoza, Lindsay Welton, Xinnie Mai, <strong>Yanwei Jin</strong>, Zidu Xu, Yuen-Hei Chung, Yiyun Xing, Meng-Han Tsai, Emma Schaffer, Yucheng Shi, Ninghao Liu, Zirui Liu, Rui Zhang"
venue: "arXiv preprint"
arxiv: https://arxiv.org/pdf/2507.07988
---
We introduce **MedThink-Bench**, a benchmark for rigorous assessment of LLMs' medical reasoning with 500 questions across ten domains, each annotated with expert step-by-step rationales. We propose **LLM-w-Ref**, a novel evaluation framework using LLM-as-a-Judge to assess reasoning with expert-level fidelity. Benchmarking twelve state-of-the-art LLMs showed smaller models (e.g., MedGemma-27B) can surpass larger proprietary ones (e.g., OpenAI-o3).
